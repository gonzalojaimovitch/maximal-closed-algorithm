{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "No9YuSzBp8sp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data -> dataframe with dataset\n",
        "# attributes -> data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ngXQ2iIct6h-"
      },
      "outputs": [],
      "source": [
        "# Discretize numerical variables\n",
        "\n",
        "continuous_types = [\"float64\"]\n",
        "numBins = 5\n",
        "\n",
        "dict_attr = {}\n",
        "dict_unique = {}\n",
        "\n",
        "for key in data:\n",
        "  dict_attr[key] = data[key].to_numpy()\n",
        "  if str(dict_attr[key].dtype) in continuous_types:\n",
        "    min = np.amin(dict_attr[key])\n",
        "    max = np.amax(dict_attr[key])\n",
        "    interval = (max - min) / numBins\n",
        "    bins = np.arange(min, max, interval)\n",
        "    dict_attr[key] = np.array([f\"{key}-{elem}\" for elem in np.digitize(dict_attr[key], bins).astype(str)])\n",
        "  dict_unique[key] = np.unique(dict_attr[key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YsnmphhF0bWs"
      },
      "outputs": [],
      "source": [
        "# The different attributes are aggregated into a set variable\n",
        "data_sets = np.apply_along_axis(set, 1, np.column_stack([dict_attr[key] for key in dict_attr.keys()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JX6w6HhjAHpk"
      },
      "outputs": [],
      "source": [
        "dict_unique = {}\n",
        "for attribute in attributes:\n",
        "  dict_unique[attribute] = attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO2OWiDm1MBx"
      },
      "source": [
        "Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Et17NGUdnGa9"
      },
      "outputs": [],
      "source": [
        "class Itemset:\n",
        "  def __init__(self, attributes):\n",
        "    self.attributes = dict.fromkeys(attributes, None)\n",
        "    self.sons = []\n",
        "\n",
        "  def add_item(self, key, item):\n",
        "    self.attributes[key] = item\n",
        "\n",
        "  def add_parent(self, parent_id):\n",
        "    self.parent = parent_id\n",
        "\n",
        "  def add_son(self, son_id):\n",
        "    self.sons.append(son_id)\n",
        "\n",
        "  def add_state(self, state):\n",
        "    self.state = state\n",
        "\n",
        "  def calc_support_rel(self, dataset):\n",
        "    total_length = len(dataset)\n",
        "    support_abs = 0\n",
        "\n",
        "    for instance in dataset:\n",
        "      if self.itemset.issubset(instance):\n",
        "        support_abs += 1\n",
        "    \n",
        "    support_res = support_abs / total_length\n",
        "\n",
        "    self.support = support_res\n",
        "\n",
        "  def generate_itemset(self):\n",
        "    self.itemset = set()\n",
        "    for key in self.attributes:\n",
        "      item = self.attributes[key]\n",
        "      if item is not None:\n",
        "        self.itemset.add(self.attributes[key])\n",
        "    self.itemset = frozenset(self.itemset)\n",
        "  \n",
        "  def written_attributes(self):\n",
        "    attr_list = []\n",
        "    for key in self.attributes:\n",
        "      if self.attributes[key] is not None:\n",
        "        attr_list.append(key)\n",
        "    return attr_list\n",
        "\n",
        "  def create_son(self):\n",
        "    created_son = deepcopy(self)\n",
        "    created_son.itemset = None\n",
        "    created_son.parent = None\n",
        "    created_son.state = None\n",
        "    created_son.support = None\n",
        "    created_son.sons = []\n",
        "    return created_son\n",
        "\n",
        "  def copy_itemset(self):\n",
        "    copied_itemset = deepcopy(self)\n",
        "    return copied_itemset\n",
        "\n",
        "  def define_support_state(self, min_support):\n",
        "    if self.support >= min_support:\n",
        "      self.state = \"frequent\"\n",
        "    else:\n",
        "      self.state = \"no_frequent\"\n",
        "  \n",
        "  def describe(self):\n",
        "    print(f\"Itemset: {self.itemset}\\nAttributes: {self.attributes}\\nSupport: {self.support}\\nState: {self.state}\\nParent: {self.parent}\\nSons: {self.sons}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PZkTPPkjKrin"
      },
      "outputs": [],
      "source": [
        "min_support = 0.9\n",
        "\n",
        "# Variable that represents the tree of itemsets\n",
        "tree = {}\n",
        "#Variable to controle the generation of duplicated itemsets\n",
        "explored_itemsets = {}\n",
        "\n",
        "maximals = []\n",
        "closed = []\n",
        "\n",
        "# Build the tree (base case)\n",
        "available_level = 0\n",
        "tree[available_level] = []\n",
        "created_nodes = 0\n",
        "for key in dict_unique:\n",
        "  for elem in dict_unique[key]:\n",
        "    itemset = Itemset(attributes)\n",
        "    itemset.add_item(key, elem)\n",
        "    itemset.add_parent(-1)\n",
        "    itemset.generate_itemset()\n",
        "    explored_itemsets[itemset.itemset] = created_nodes\n",
        "    itemset.calc_support_rel(data_sets)\n",
        "    itemset.define_support_state(min_support)\n",
        "    tree[available_level].append(itemset)\n",
        "    created_nodes += 1\n",
        "    \n",
        "available_level = 1\n",
        "\n",
        "\n",
        "# Build the tree (general case)\n",
        "for iter in range(1, len(attributes)):\n",
        "  tree[available_level] = []\n",
        "  created_nodes = 0\n",
        "  # Itemsets of the previous level are checked to generate itemsets from the frequent itemsets\n",
        "  for id, itemset in enumerate(tree[available_level-1]):\n",
        "    if itemset.state == \"frequent\":\n",
        "      for attribute in [attribute for attribute in attributes if attribute not in itemset.written_attributes()]:\n",
        "        for item in dict_unique[attribute]:\n",
        "          check_set = set(deepcopy(itemset.itemset))\n",
        "          check_set.add(item)\n",
        "          if frozenset(check_set) not in list(explored_itemsets.keys()):\n",
        "            super_itemset = itemset.create_son()\n",
        "            itemset.add_son(created_nodes)\n",
        "            super_itemset.add_parent(id)\n",
        "            super_itemset.add_item(attribute, item)\n",
        "            super_itemset.generate_itemset()\n",
        "            explored_itemsets[super_itemset.itemset] = created_nodes\n",
        "            super_itemset.calc_support_rel(data_sets)\n",
        "            super_itemset.define_support_state(min_support)\n",
        "            tree[available_level].append(super_itemset)\n",
        "            created_nodes += 1\n",
        "          else:\n",
        "            itemset.add_son(explored_itemsets[frozenset(check_set)])\n",
        "\n",
        "  available_level += 1  \n",
        "\n",
        "# Obtain maximal\n",
        "for level in tree:\n",
        "  for itemset in tree[level]:\n",
        "    if itemset.state == \"frequent\":\n",
        "      total_sons = len(itemset.sons)\n",
        "      no_frequent_sons = 0\n",
        "      for son in itemset.sons:\n",
        "        if tree[level + 1][son].state == \"no_frequent\":\n",
        "          no_frequent_sons += 1\n",
        "      if total_sons == no_frequent_sons:\n",
        "        maximals.append(set(itemset.itemset))\n",
        "\n",
        "# Obtain closed\n",
        "for level in tree:\n",
        "  for itemset in tree[level]:\n",
        "    isClosed = True\n",
        "    if len(itemset.sons) == 0:\n",
        "      isClosed = False\n",
        "    else:\n",
        "      for son in itemset.sons:\n",
        "        if round(tree[level + 1][son].support, 4) == round(itemset.support, 4):\n",
        "          isClosed = False\n",
        "          break\n",
        "    if isClosed == True:\n",
        "      closed.append(set(itemset.itemset))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MaximalAndClosedAlgorithms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('brain_stroke')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ddfdb6686e77c65da236fbcea4e097a9505070c7600d902ea8b7a89a9f7a7514"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
